{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMcT0CWo-icE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-qmZNEspiNf",
        "outputId": "e13a2174-7dc3-4492-afb1-febaf51ebfd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading OCEAN Ground Truth Scores ---\n",
            "‚úÖ Loaded 1000 videos with OCEAN scores.\n",
            "   Assumed order: O(0), C(1), E(2), A(3), N(4)\n",
            "Applying binary mapping using a threshold of 0.5...\n",
            "\n",
            "--- Mapping Complete ---\n",
            "MBTI Label array shape: (1000, 4)\n",
            "Output MBTI order: [E/I, S/N, T/F, J/P]\n",
            "Video 1 Labels: [0. 0. 0. 1.]\n",
            "Saved MBTI labels to: /content/drive/MyDrive/NNDL/Features/seq_y_mbti.npy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- Define Paths ---\n",
        "FEATURE_DIR = \"/content/drive/MyDrive/NNDL/Features\"\n",
        "\n",
        "# Path to your final 1000-row OCEAN score ground truth array (the input for the mapping)\n",
        "y_ocean_path = os.path.join(FEATURE_DIR, \"seq_y.npy\")\n",
        "\n",
        "# Output path for the new 4-trait MBTI binary label array\n",
        "mbti_y_path = os.path.join(FEATURE_DIR, \"seq_y_mbti.npy\")\n",
        "\n",
        "# --- 1. Load OCEAN Ground Truth ---\n",
        "print(\"--- Loading OCEAN Ground Truth Scores ---\")\n",
        "try:\n",
        "    # Load the (1000, 5) array of true OCEAN scores\n",
        "    seq_y_ocean = np.load(y_ocean_path)\n",
        "    N = len(seq_y_ocean)\n",
        "    if seq_y_ocean.shape[1] != 5:\n",
        "        raise ValueError(f\"Expected 5 columns, found {seq_y_ocean.shape[1]}\")\n",
        "\n",
        "    # üö® CRITICAL CHECK: Determine the order of the 5 traits in your array\n",
        "    # We assume the order is: [Openness (O), Conscientiousness (C), Extraversion (E), Agreeableness (A), Neuroticism (N)]\n",
        "    O_IDX = 0 # Maps to N/S\n",
        "    C_IDX = 1 # Maps to J/P\n",
        "    E_IDX = 2 # Maps to E/I\n",
        "    A_IDX = 3 # Maps to F/T (Feeling/Thinking)\n",
        "    N_IDX = 4 # Not used in this primary mapping\n",
        "\n",
        "    print(f\"‚úÖ Loaded {N} videos with OCEAN scores.\")\n",
        "    print(f\"   Assumed order: O({O_IDX}), C({C_IDX}), E({E_IDX}), A({A_IDX}), N({N_IDX})\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR loading OCEAN data from {y_ocean_path}: {e}. Exiting.\")\n",
        "    sys.exit()\n",
        "\n",
        "# --- 2. Define Mapping Parameters ---\n",
        "# MBTI output order: [E/I, S/N, T/F, J/P]\n",
        "mbti_labels = np.zeros((N, 4), dtype=np.float32)\n",
        "THRESHOLD = 0.5\n",
        "print(f\"Applying binary mapping using a threshold of {THRESHOLD}...\")\n",
        "\n",
        "# --- 3. Apply Threshold Mapping ---\n",
        "\n",
        "# 1. E/I (Extraversion/Introversion) <-> Extraversion (E)\n",
        "# High E (score > 0.5) maps to E (1). Low E maps to I (0).\n",
        "mbti_labels[:, 0] = (seq_y_ocean[:, E_IDX] > THRESHOLD).astype(np.float32)\n",
        "# Trait 1 (MBTI Index 0): E (1) or I (0)\n",
        "\n",
        "# 2. S/N (Sensing/Intuition) <-> Openness (O)\n",
        "# High O (score > 0.5) maps to N (Intuition, 1). Low O maps to S (Sensing, 0).\n",
        "mbti_labels[:, 1] = (seq_y_ocean[:, O_IDX] > THRESHOLD).astype(np.float32)\n",
        "# Trait 2 (MBTI Index 1): N (1) or S (0)\n",
        "\n",
        "# 3. T/F (Thinking/Feeling) <-> Agreeableness (A)\n",
        "# High A (score > 0.5) maps to F (Feeling, 1). Low A maps to T (Thinking, 0).\n",
        "mbti_labels[:, 2] = (seq_y_ocean[:, A_IDX] > THRESHOLD).astype(np.float32)\n",
        "# Trait 3 (MBTI Index 2): F (1) or T (0)\n",
        "\n",
        "# 4. J/P (Judging/Perceiving) <-> Conscientiousness (C)\n",
        "# High C (score > 0.5) maps to J (Judging, 1). Low C maps to P (Perceiving, 0).\n",
        "mbti_labels[:, 3] = (seq_y_ocean[:, C_IDX] > THRESHOLD).astype(np.float32)\n",
        "# Trait 4 (MBTI Index 3): J (1) or P (0)\n",
        "\n",
        "\n",
        "# --- 4. Save the MBTI Ground Truth ---\n",
        "np.save(mbti_y_path, mbti_labels)\n",
        "\n",
        "print(\"\\n--- Mapping Complete ---\")\n",
        "print(f\"MBTI Label array shape: {mbti_labels.shape}\")\n",
        "print(f\"Output MBTI order: [E/I, S/N, T/F, J/P]\")\n",
        "print(f\"Video 1 Labels: {mbti_labels[0]}\")\n",
        "print(f\"Saved MBTI labels to: {mbti_y_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IqBfMJeKOsPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# --- Define Paths ---\n",
        "FEATURE_DIR = \"/content/drive/MyDrive/NNDL/Features\"\n",
        "\n",
        "# Input X for the MLP is the 5-dim OCEAN Ground Truth\n",
        "y_ocean_path = os.path.join(FEATURE_DIR, \"seq_y.npy\")\n",
        "# Target Y for the MLP is the 4-dim MBTI Ground Truth\n",
        "mbti_y_path = os.path.join(FEATURE_DIR, \"seq_y_mbti.npy\")\n",
        "\n",
        "MODEL_SAVE_PATH_MBTI = os.path.join(FEATURE_DIR, \"best_mbti_mlp.pth\") # Final MLP Checkpoint\n",
        "\n",
        "# --- 2. Load Data ---\n",
        "print(\"--- Loading OCEAN Input and MBTI Target Data ---\")\n",
        "try:\n",
        "    seq_y_ocean = np.load(y_ocean_path)\n",
        "    mbti_labels = np.load(mbti_y_path)\n",
        "    N = len(seq_y_ocean)\n",
        "\n",
        "    if seq_y_ocean.shape != (N, 5) or mbti_labels.shape != (N, 4):\n",
        "         raise ValueError(f\"Shape mismatch: OCEAN {seq_y_ocean.shape}, MBTI {mbti_labels.shape}\")\n",
        "\n",
        "    print(f\"‚úÖ Data loaded successfully. N={N} sequences.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading data: {e}. Exiting.\")\n",
        "    sys.exit()\n",
        "\n",
        "# --- 3. Prepare Tensors and Split ---\n",
        "ocean_input_tensor = torch.tensor(seq_y_ocean, dtype=torch.float32) # (1000, 5)\n",
        "mbti_target_tensor = torch.tensor(mbti_labels, dtype=torch.float32)  # (1000, 4)\n",
        "\n",
        "dataset = TensorDataset(ocean_input_tensor, mbti_target_tensor)\n",
        "\n",
        "train_size = int(0.7 * N)\n",
        "val_size   = int(0.15 * N)\n",
        "test_size  = N - train_size - val_size\n",
        "\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
        "\n",
        "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n",
        "\n",
        "# --- 4. Define the Small MLP Classifier ---\n",
        "class MBTIMLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=5, output_dim=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = MBTIMLPClassifier(input_dim=5, output_dim=4).to(DEVICE)\n",
        "criterion = nn.BCELoss() # Binary Cross-Entropy Loss\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=20e-4, weight_decay=1e-5)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 10\n",
        "pat_ctr = 0\n",
        "EPOCHS = 100\n",
        "\n",
        "print(f\"\\nStarting MLP training on {DEVICE}...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "\n",
        "    # -------- Validation --------\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    y_true_val, y_pred_val = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "            val_loss += loss.item() * x.size(0)\n",
        "\n",
        "            y_true_val.append(y.cpu().numpy())\n",
        "            y_pred_val.append((preds > 0.5).float().cpu().numpy())\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_ds)\n",
        "    avg_val_loss = val_loss / len(val_ds)\n",
        "\n",
        "    y_true_val = np.vstack(y_true_val)\n",
        "    y_pred_val = np.vstack(y_pred_val)\n",
        "\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "    val_f1_macro = f1_score(y_true_val, y_pred_val, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n",
        "          f\"Val Acc: {val_accuracy:.4f} | Val F1 (Macro): {val_f1_macro:.4f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_state = model.state_dict()\n",
        "        pat_ctr = 0\n",
        "        torch.save(best_state, MODEL_SAVE_PATH_MBTI)\n",
        "        # print(f\"¬†¬† üíæ Best MLP model saved.\")\n",
        "    else:\n",
        "        pat_ctr += 1\n",
        "        if pat_ctr >= patience:\n",
        "            print(\"‚èπ Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_state)\n",
        "\n",
        "# --- 6. Final Test Evaluation ---\n",
        "model.eval()\n",
        "true_list, pred_list = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        preds = model(x)\n",
        "\n",
        "        true_list.append(y.cpu().numpy())\n",
        "        pred_list.append((preds > 0.5).float().cpu().numpy())\n",
        "\n",
        "y_true_test = np.vstack(true_list)\n",
        "y_pred_test = np.vstack(pred_list)\n",
        "\n",
        "test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
        "test_f1_macro = f1_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
        "test_precision = precision_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
        "test_recall = recall_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
        "\n",
        "\n",
        "print(\"\\n================= FINAL MBTI MLP TEST RESULTS =================\")\n",
        "print(f\"Accuracy (Total Label Match): {test_accuracy:.4f}\")\n",
        "print(f\"F1 Score (Macro, per trait): {test_f1_macro:.4f}\")\n",
        "print(\"=============================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zeHlSiB3CsN",
        "outputId": "898844a7-40e3-436a-9398-282abf0989aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading OCEAN Input and MBTI Target Data ---\n",
            "‚úÖ Data loaded successfully. N=1000 sequences.\n",
            "Train: 700 | Val: 150 | Test: 150\n",
            "\n",
            "Starting MLP training on cpu...\n",
            "Epoch 01 | Train Loss: 0.6845 | Val Loss: 0.6728 | Val Acc: 0.1067 | Val F1 (Macro): 0.5714\n",
            "Epoch 02 | Train Loss: 0.6650 | Val Loss: 0.6436 | Val Acc: 0.2067 | Val F1 (Macro): 0.6688\n",
            "Epoch 03 | Train Loss: 0.6304 | Val Loss: 0.6044 | Val Acc: 0.3267 | Val F1 (Macro): 0.7727\n",
            "Epoch 04 | Train Loss: 0.5936 | Val Loss: 0.5523 | Val Acc: 0.3533 | Val F1 (Macro): 0.8120\n",
            "Epoch 05 | Train Loss: 0.5447 | Val Loss: 0.4889 | Val Acc: 0.5067 | Val F1 (Macro): 0.8501\n",
            "Epoch 06 | Train Loss: 0.4945 | Val Loss: 0.4293 | Val Acc: 0.5200 | Val F1 (Macro): 0.8650\n",
            "Epoch 07 | Train Loss: 0.4493 | Val Loss: 0.3901 | Val Acc: 0.5333 | Val F1 (Macro): 0.8711\n",
            "Epoch 08 | Train Loss: 0.4191 | Val Loss: 0.3604 | Val Acc: 0.5400 | Val F1 (Macro): 0.8778\n",
            "Epoch 09 | Train Loss: 0.3967 | Val Loss: 0.3482 | Val Acc: 0.5200 | Val F1 (Macro): 0.8757\n",
            "Epoch 10 | Train Loss: 0.3872 | Val Loss: 0.3351 | Val Acc: 0.5867 | Val F1 (Macro): 0.8865\n",
            "Epoch 11 | Train Loss: 0.3819 | Val Loss: 0.3308 | Val Acc: 0.5800 | Val F1 (Macro): 0.8832\n",
            "Epoch 12 | Train Loss: 0.3725 | Val Loss: 0.3266 | Val Acc: 0.5667 | Val F1 (Macro): 0.8830\n",
            "Epoch 13 | Train Loss: 0.3692 | Val Loss: 0.3240 | Val Acc: 0.5533 | Val F1 (Macro): 0.8816\n",
            "Epoch 14 | Train Loss: 0.3681 | Val Loss: 0.3218 | Val Acc: 0.5667 | Val F1 (Macro): 0.8811\n",
            "Epoch 15 | Train Loss: 0.3734 | Val Loss: 0.3203 | Val Acc: 0.5667 | Val F1 (Macro): 0.8842\n",
            "Epoch 16 | Train Loss: 0.3620 | Val Loss: 0.3207 | Val Acc: 0.5533 | Val F1 (Macro): 0.8787\n",
            "Epoch 17 | Train Loss: 0.3660 | Val Loss: 0.3184 | Val Acc: 0.5533 | Val F1 (Macro): 0.8835\n",
            "Epoch 18 | Train Loss: 0.3589 | Val Loss: 0.3153 | Val Acc: 0.5533 | Val F1 (Macro): 0.8803\n",
            "Epoch 19 | Train Loss: 0.3515 | Val Loss: 0.3085 | Val Acc: 0.5667 | Val F1 (Macro): 0.8841\n",
            "Epoch 20 | Train Loss: 0.3530 | Val Loss: 0.3076 | Val Acc: 0.5533 | Val F1 (Macro): 0.8789\n",
            "Epoch 21 | Train Loss: 0.3501 | Val Loss: 0.3038 | Val Acc: 0.5733 | Val F1 (Macro): 0.8841\n",
            "Epoch 22 | Train Loss: 0.3471 | Val Loss: 0.3000 | Val Acc: 0.5800 | Val F1 (Macro): 0.8853\n",
            "Epoch 23 | Train Loss: 0.3425 | Val Loss: 0.2959 | Val Acc: 0.5867 | Val F1 (Macro): 0.8921\n",
            "Epoch 24 | Train Loss: 0.3380 | Val Loss: 0.2920 | Val Acc: 0.5733 | Val F1 (Macro): 0.8854\n",
            "Epoch 25 | Train Loss: 0.3329 | Val Loss: 0.2886 | Val Acc: 0.5733 | Val F1 (Macro): 0.8865\n",
            "Epoch 26 | Train Loss: 0.3260 | Val Loss: 0.2826 | Val Acc: 0.6133 | Val F1 (Macro): 0.8946\n",
            "Epoch 27 | Train Loss: 0.3238 | Val Loss: 0.2783 | Val Acc: 0.6133 | Val F1 (Macro): 0.8957\n",
            "Epoch 28 | Train Loss: 0.3312 | Val Loss: 0.2769 | Val Acc: 0.6000 | Val F1 (Macro): 0.8964\n",
            "Epoch 29 | Train Loss: 0.3128 | Val Loss: 0.2686 | Val Acc: 0.6467 | Val F1 (Macro): 0.9057\n",
            "Epoch 30 | Train Loss: 0.3051 | Val Loss: 0.2649 | Val Acc: 0.6467 | Val F1 (Macro): 0.9066\n",
            "Epoch 31 | Train Loss: 0.3041 | Val Loss: 0.2602 | Val Acc: 0.6600 | Val F1 (Macro): 0.9112\n",
            "Epoch 32 | Train Loss: 0.3058 | Val Loss: 0.2583 | Val Acc: 0.6267 | Val F1 (Macro): 0.9034\n",
            "Epoch 33 | Train Loss: 0.3055 | Val Loss: 0.2608 | Val Acc: 0.6267 | Val F1 (Macro): 0.9052\n",
            "Epoch 34 | Train Loss: 0.2989 | Val Loss: 0.2509 | Val Acc: 0.6867 | Val F1 (Macro): 0.9155\n",
            "Epoch 35 | Train Loss: 0.2954 | Val Loss: 0.2437 | Val Acc: 0.6800 | Val F1 (Macro): 0.9150\n",
            "Epoch 36 | Train Loss: 0.2892 | Val Loss: 0.2465 | Val Acc: 0.6600 | Val F1 (Macro): 0.9101\n",
            "Epoch 37 | Train Loss: 0.2846 | Val Loss: 0.2377 | Val Acc: 0.6933 | Val F1 (Macro): 0.9184\n",
            "Epoch 38 | Train Loss: 0.2882 | Val Loss: 0.2373 | Val Acc: 0.6933 | Val F1 (Macro): 0.9191\n",
            "Epoch 39 | Train Loss: 0.2841 | Val Loss: 0.2335 | Val Acc: 0.6800 | Val F1 (Macro): 0.9162\n",
            "Epoch 40 | Train Loss: 0.2739 | Val Loss: 0.2353 | Val Acc: 0.6933 | Val F1 (Macro): 0.9211\n",
            "Epoch 41 | Train Loss: 0.2751 | Val Loss: 0.2301 | Val Acc: 0.7000 | Val F1 (Macro): 0.9223\n",
            "Epoch 42 | Train Loss: 0.2779 | Val Loss: 0.2295 | Val Acc: 0.6933 | Val F1 (Macro): 0.9186\n",
            "Epoch 43 | Train Loss: 0.2710 | Val Loss: 0.2255 | Val Acc: 0.6867 | Val F1 (Macro): 0.9186\n",
            "Epoch 44 | Train Loss: 0.2693 | Val Loss: 0.2245 | Val Acc: 0.6867 | Val F1 (Macro): 0.9182\n",
            "Epoch 45 | Train Loss: 0.2672 | Val Loss: 0.2211 | Val Acc: 0.7000 | Val F1 (Macro): 0.9204\n",
            "Epoch 46 | Train Loss: 0.2691 | Val Loss: 0.2223 | Val Acc: 0.6933 | Val F1 (Macro): 0.9183\n",
            "Epoch 47 | Train Loss: 0.2634 | Val Loss: 0.2178 | Val Acc: 0.7067 | Val F1 (Macro): 0.9216\n",
            "Epoch 48 | Train Loss: 0.2597 | Val Loss: 0.2218 | Val Acc: 0.6733 | Val F1 (Macro): 0.9152\n",
            "Epoch 49 | Train Loss: 0.2562 | Val Loss: 0.2172 | Val Acc: 0.6867 | Val F1 (Macro): 0.9184\n",
            "Epoch 50 | Train Loss: 0.2584 | Val Loss: 0.2208 | Val Acc: 0.6733 | Val F1 (Macro): 0.9152\n",
            "Epoch 51 | Train Loss: 0.2637 | Val Loss: 0.2101 | Val Acc: 0.7133 | Val F1 (Macro): 0.9252\n",
            "Epoch 52 | Train Loss: 0.2613 | Val Loss: 0.2100 | Val Acc: 0.7000 | Val F1 (Macro): 0.9200\n",
            "Epoch 53 | Train Loss: 0.2538 | Val Loss: 0.2106 | Val Acc: 0.6867 | Val F1 (Macro): 0.9188\n",
            "Epoch 54 | Train Loss: 0.2526 | Val Loss: 0.2062 | Val Acc: 0.7067 | Val F1 (Macro): 0.9216\n",
            "Epoch 55 | Train Loss: 0.2508 | Val Loss: 0.2052 | Val Acc: 0.7067 | Val F1 (Macro): 0.9240\n",
            "Epoch 56 | Train Loss: 0.2492 | Val Loss: 0.2040 | Val Acc: 0.7133 | Val F1 (Macro): 0.9250\n",
            "Epoch 57 | Train Loss: 0.2364 | Val Loss: 0.1970 | Val Acc: 0.7133 | Val F1 (Macro): 0.9226\n",
            "Epoch 58 | Train Loss: 0.2372 | Val Loss: 0.2008 | Val Acc: 0.6933 | Val F1 (Macro): 0.9199\n",
            "Epoch 59 | Train Loss: 0.2476 | Val Loss: 0.1973 | Val Acc: 0.7067 | Val F1 (Macro): 0.9234\n",
            "Epoch 60 | Train Loss: 0.2394 | Val Loss: 0.1965 | Val Acc: 0.7200 | Val F1 (Macro): 0.9270\n",
            "Epoch 61 | Train Loss: 0.2361 | Val Loss: 0.1917 | Val Acc: 0.7200 | Val F1 (Macro): 0.9263\n",
            "Epoch 62 | Train Loss: 0.2375 | Val Loss: 0.1945 | Val Acc: 0.7000 | Val F1 (Macro): 0.9213\n",
            "Epoch 63 | Train Loss: 0.2320 | Val Loss: 0.1872 | Val Acc: 0.7200 | Val F1 (Macro): 0.9279\n",
            "Epoch 64 | Train Loss: 0.2233 | Val Loss: 0.1881 | Val Acc: 0.7200 | Val F1 (Macro): 0.9268\n",
            "Epoch 65 | Train Loss: 0.2317 | Val Loss: 0.1887 | Val Acc: 0.7333 | Val F1 (Macro): 0.9301\n",
            "Epoch 66 | Train Loss: 0.2313 | Val Loss: 0.1796 | Val Acc: 0.7533 | Val F1 (Macro): 0.9340\n",
            "Epoch 67 | Train Loss: 0.2252 | Val Loss: 0.1842 | Val Acc: 0.7133 | Val F1 (Macro): 0.9280\n",
            "Epoch 68 | Train Loss: 0.2160 | Val Loss: 0.1791 | Val Acc: 0.7000 | Val F1 (Macro): 0.9254\n",
            "Epoch 69 | Train Loss: 0.2221 | Val Loss: 0.1730 | Val Acc: 0.7600 | Val F1 (Macro): 0.9390\n",
            "Epoch 70 | Train Loss: 0.2146 | Val Loss: 0.1818 | Val Acc: 0.7400 | Val F1 (Macro): 0.9314\n",
            "Epoch 71 | Train Loss: 0.2156 | Val Loss: 0.1679 | Val Acc: 0.7533 | Val F1 (Macro): 0.9361\n",
            "Epoch 72 | Train Loss: 0.2162 | Val Loss: 0.1748 | Val Acc: 0.7400 | Val F1 (Macro): 0.9332\n",
            "Epoch 73 | Train Loss: 0.2050 | Val Loss: 0.1651 | Val Acc: 0.7600 | Val F1 (Macro): 0.9409\n",
            "Epoch 74 | Train Loss: 0.2078 | Val Loss: 0.1584 | Val Acc: 0.7867 | Val F1 (Macro): 0.9465\n",
            "Epoch 75 | Train Loss: 0.1996 | Val Loss: 0.1595 | Val Acc: 0.7733 | Val F1 (Macro): 0.9432\n",
            "Epoch 76 | Train Loss: 0.2074 | Val Loss: 0.1657 | Val Acc: 0.7533 | Val F1 (Macro): 0.9383\n",
            "Epoch 77 | Train Loss: 0.2028 | Val Loss: 0.1599 | Val Acc: 0.7733 | Val F1 (Macro): 0.9429\n",
            "Epoch 78 | Train Loss: 0.2061 | Val Loss: 0.1543 | Val Acc: 0.8000 | Val F1 (Macro): 0.9476\n",
            "Epoch 79 | Train Loss: 0.1906 | Val Loss: 0.1494 | Val Acc: 0.8000 | Val F1 (Macro): 0.9499\n",
            "Epoch 80 | Train Loss: 0.1887 | Val Loss: 0.1515 | Val Acc: 0.7733 | Val F1 (Macro): 0.9423\n",
            "Epoch 81 | Train Loss: 0.1977 | Val Loss: 0.1538 | Val Acc: 0.7800 | Val F1 (Macro): 0.9460\n",
            "Epoch 82 | Train Loss: 0.1929 | Val Loss: 0.1459 | Val Acc: 0.7867 | Val F1 (Macro): 0.9460\n",
            "Epoch 83 | Train Loss: 0.1890 | Val Loss: 0.1503 | Val Acc: 0.7533 | Val F1 (Macro): 0.9388\n",
            "Epoch 84 | Train Loss: 0.1881 | Val Loss: 0.1428 | Val Acc: 0.8200 | Val F1 (Macro): 0.9550\n",
            "Epoch 85 | Train Loss: 0.1910 | Val Loss: 0.1443 | Val Acc: 0.7733 | Val F1 (Macro): 0.9445\n",
            "Epoch 86 | Train Loss: 0.1888 | Val Loss: 0.1394 | Val Acc: 0.8067 | Val F1 (Macro): 0.9508\n",
            "Epoch 87 | Train Loss: 0.1857 | Val Loss: 0.1365 | Val Acc: 0.8400 | Val F1 (Macro): 0.9609\n",
            "Epoch 88 | Train Loss: 0.1844 | Val Loss: 0.1379 | Val Acc: 0.8000 | Val F1 (Macro): 0.9493\n",
            "Epoch 89 | Train Loss: 0.1822 | Val Loss: 0.1365 | Val Acc: 0.8000 | Val F1 (Macro): 0.9498\n",
            "Epoch 90 | Train Loss: 0.1707 | Val Loss: 0.1340 | Val Acc: 0.8200 | Val F1 (Macro): 0.9544\n",
            "Epoch 91 | Train Loss: 0.1726 | Val Loss: 0.1319 | Val Acc: 0.8133 | Val F1 (Macro): 0.9531\n",
            "Epoch 92 | Train Loss: 0.1834 | Val Loss: 0.1363 | Val Acc: 0.7667 | Val F1 (Macro): 0.9447\n",
            "Epoch 93 | Train Loss: 0.1806 | Val Loss: 0.1271 | Val Acc: 0.8533 | Val F1 (Macro): 0.9613\n",
            "Epoch 94 | Train Loss: 0.1748 | Val Loss: 0.1301 | Val Acc: 0.8267 | Val F1 (Macro): 0.9576\n",
            "Epoch 95 | Train Loss: 0.1693 | Val Loss: 0.1267 | Val Acc: 0.8667 | Val F1 (Macro): 0.9664\n",
            "Epoch 96 | Train Loss: 0.1618 | Val Loss: 0.1258 | Val Acc: 0.8400 | Val F1 (Macro): 0.9643\n",
            "Epoch 97 | Train Loss: 0.1657 | Val Loss: 0.1228 | Val Acc: 0.8400 | Val F1 (Macro): 0.9594\n",
            "Epoch 98 | Train Loss: 0.1627 | Val Loss: 0.1231 | Val Acc: 0.8467 | Val F1 (Macro): 0.9623\n",
            "Epoch 99 | Train Loss: 0.1631 | Val Loss: 0.1296 | Val Acc: 0.7933 | Val F1 (Macro): 0.9535\n",
            "Epoch 100 | Train Loss: 0.1637 | Val Loss: 0.1218 | Val Acc: 0.8333 | Val F1 (Macro): 0.9640\n",
            "\n",
            "================= FINAL MBTI MLP TEST RESULTS =================\n",
            "Accuracy (Total Label Match): 0.8400\n",
            "F1 Score (Macro, per trait): 0.9655\n",
            "=============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# ====================================================================\n",
        "# ‚ö†Ô∏è NECESSARY CLASS DEFINITIONS\n",
        "# ====================================================================\n",
        "\n",
        "# 1. OceanDataset (The data wrapper for splitting)\n",
        "class OceanDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper for sequence data, only used here to facilitate the random_split.\"\"\"\n",
        "    def __init__(self, X, Y, IDs):\n",
        "        self.X = X\n",
        "        self.Y = Y # Dummy array for split\n",
        "        self.IDs = IDs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return as NumPy arrays/strings for collate_fn to handle padding and Tensors\n",
        "        return {\"x\": self.X[idx], \"y\": self.Y[idx], \"id\": self.IDs[idx]}\n",
        "\n",
        "# 2. collate_fn (The padding function for DataLoader)\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Pads sequences in a batch to the maximum length and creates a mask.\"\"\"\n",
        "    # Separate sequences and non-sequence data\n",
        "    sequences = [item[\"x\"] for item in batch]\n",
        "    ids = [item[\"id\"] for item in batch]\n",
        "\n",
        "    # Determine max length\n",
        "    max_len = max(len(s) for s in sequences)\n",
        "\n",
        "    # Pad sequences and create mask\n",
        "    padded_sequences = []\n",
        "    masks = []\n",
        "    for seq in sequences:\n",
        "        # Pad with zeros\n",
        "        padding_needed = max_len - len(seq)\n",
        "        padded_seq = np.pad(seq, ((0, padding_needed), (0, 0)), mode='constant', constant_values=0.0)\n",
        "        padded_sequences.append(padded_seq)\n",
        "\n",
        "        # Create mask: 1 for real data, 0 for padding\n",
        "        mask = np.zeros(max_len, dtype=np.bool_)\n",
        "        mask[:len(seq)] = 1\n",
        "        masks.append(mask)\n",
        "\n",
        "    # Convert to Tensors\n",
        "    x = torch.tensor(np.stack(padded_sequences), dtype=torch.float32) # Shape: (B, T, D)\n",
        "    mask = torch.tensor(np.stack(masks), dtype=torch.bool)          # Shape: (B, T)\n",
        "\n",
        "    return {\"x\": x, \"mask\": mask, \"id\": ids}\n",
        "\n",
        "\n",
        "# 3. OceanEncBiGRUTransformer (Phase 1 Model - OCEAN Score Regression)\n",
        "# Using the BiGRU version which successfully loaded previously with the given checkpoint.\n",
        "class OceanEncBiGRUTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim=1291, # Default is based on common multimodal features\n",
        "        clip_dim=512, fer_dim=7, wav_dim=768, prosody_dim=4, # Assumed feature dims\n",
        "        clip_enc_dim=128, fer_enc_dim=32, wav_enc_dim=128, prosody_enc_dim=16,\n",
        "        d_model=256, nhead=8, num_layers=2, dim_feedforward=512, dropout=0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.clip_dim = clip_dim\n",
        "        self.fer_dim = fer_dim\n",
        "        self.wav_dim = wav_dim\n",
        "        self.prosody_dim = prosody_dim\n",
        "\n",
        "        # -------------------------\n",
        "        # üîπ Modality Encoders\n",
        "        # -------------------------\n",
        "        self.clip_encoder = nn.Sequential(\n",
        "            nn.Linear(clip_dim, clip_enc_dim), nn.ReLU(), nn.LayerNorm(clip_enc_dim),\n",
        "        )\n",
        "        self.fer_encoder = nn.Sequential(\n",
        "            nn.Linear(fer_dim, fer_enc_dim), nn.ReLU(), nn.LayerNorm(fer_enc_dim),\n",
        "        )\n",
        "        self.wav_encoder = nn.Sequential(\n",
        "            nn.Linear(wav_dim, wav_enc_dim), nn.ReLU(), nn.LayerNorm(wav_enc_dim),\n",
        "        )\n",
        "        self.prosody_encoder = nn.Sequential(\n",
        "            nn.Linear(prosody_dim, prosody_enc_dim), nn.ReLU(), nn.LayerNorm(prosody_enc_dim),\n",
        "        )\n",
        "\n",
        "        fused_dim = clip_enc_dim + fer_enc_dim + wav_enc_dim + prosody_enc_dim\n",
        "\n",
        "        self.fuse_proj = nn.Sequential(\n",
        "            nn.Linear(fused_dim, d_model), nn.ReLU(), nn.LayerNorm(d_model),\n",
        "        )\n",
        "\n",
        "        # -------------------------\n",
        "        # üîπ BiGRU\n",
        "        # -------------------------\n",
        "        self.bigru = nn.GRU(\n",
        "            input_size=d_model, hidden_size=d_model // 2, num_layers=1, batch_first=True, bidirectional=True,\n",
        "        )\n",
        "        self.gru_ln = nn.LayerNorm(d_model)\n",
        "\n",
        "        # -------------------------\n",
        "        # üîπ Transformer\n",
        "        # -------------------------\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "\n",
        "        layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
        "\n",
        "        # -------------------------\n",
        "        # üîπ Regression Head\n",
        "        # -------------------------\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, 5), # 5 continuous OCEAN scores\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        # Split modalities (assuming the feature order from the training code)\n",
        "        x_clip = x[:, :, 0:self.clip_dim]\n",
        "        x_fer = x[:, :, self.clip_dim:self.clip_dim+self.fer_dim]\n",
        "        x_wav = x[:, :, self.clip_dim+self.fer_dim:self.clip_dim+self.fer_dim+self.wav_dim]\n",
        "        x_pros = x[:, :, -self.prosody_dim:]\n",
        "\n",
        "        # Encode modalities\n",
        "        e_clip = self.clip_encoder(x_clip)\n",
        "        e_fer = self.fer_encoder(x_fer)\n",
        "        e_wav = self.wav_encoder(x_wav)\n",
        "        e_pros = self.prosody_encoder(x_pros)\n",
        "\n",
        "        fused = torch.cat([e_clip, e_fer, e_wav, e_pros], dim=-1)\n",
        "        fused = self.fuse_proj(fused)\n",
        "\n",
        "        # BiGRU\n",
        "        gru_out, _ = self.bigru(fused)\n",
        "        gru_out = self.gru_ln(gru_out)\n",
        "\n",
        "        # CLS + Transformer\n",
        "        cls_tok = self.cls_token.expand(B, 1, -1)\n",
        "        seq = torch.cat([cls_tok, gru_out], dim=1)\n",
        "\n",
        "        cls_mask = torch.ones(B, 1, device=mask.device)\n",
        "        full_mask = torch.cat([cls_mask, mask], dim=1)\n",
        "        key_mask = (full_mask == 0) # True indicates padding\n",
        "\n",
        "        enc = self.encoder(seq, src_key_padding_mask=key_mask)\n",
        "        cls_out = enc[:, 0] # Take the output of the CLS token\n",
        "\n",
        "        # Predict OCEAN\n",
        "        return self.head(cls_out)\n",
        "\n",
        "\n",
        "# 4. MBTIMLPClassifier (Phase 2 Model - MBTI Classification)\n",
        "# ‚ö†Ô∏è FIXED: hidden_dim changed to 32 and one Dropout layer removed to match the saved checkpoint architecture.\n",
        "class MBTIMLPClassifier(nn.Module):\n",
        "    \"\"\"Phase 2 Model: Takes 5 OCEAN scores and predicts 4 MBTI traits.\"\"\"\n",
        "    def __init__(self, input_dim=5, output_dim=4, hidden_dim=32, dropout=0.5): # hidden_dim=32 fixed\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),         # net.0: (5, 32)\n",
        "            nn.ReLU(),                                # net.1\n",
        "            nn.Dropout(dropout),                      # net.2\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),   # net.3: (32, 16)\n",
        "            nn.ReLU(),                                # net.4\n",
        "            # nn.Dropout(dropout),                    # net.5: REMOVED (This was the mismatch)\n",
        "            nn.Linear(hidden_dim // 2, output_dim),   # net.5: (16, 4) <-- Index shifted to 5\n",
        "            nn.Sigmoid()                              # net.6 <-- Index shifted to 6\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# --- 1. Define Final Test Paths and Parameters ---\n",
        "# --------------------------------------------------------------------\n",
        "FEATURE_DIR = \"/content/drive/MyDrive/NNDL/Features\"\n",
        "\n",
        "# Data paths for Test Loader creation (as per original script)\n",
        "X_final_path = os.path.join(FEATURE_DIR, \"seq_X.npy\")\n",
        "mbti_y_path = os.path.join(FEATURE_DIR, \"seq_y_mbti.npy\")\n",
        "seq_id_path = os.path.join(FEATURE_DIR, \"seq_id.npy\")\n",
        "\n",
        "# Model checkpoint paths - Using the uploaded file names\n",
        "P1_MODEL_PATH = \"/content/drive/MyDrive/NNDL/Features/ocean_enc_bigru_trans_best.pth\" # Phase 1 (OCEAN)\n",
        "P2_MODEL_PATH = \"/content/drive/MyDrive/NNDL/Features/best_mbti_mlp(1).pth\"# Phase 2 (MBTI)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# --- 2. Load Test Data & Prepare DataLoader ---\n",
        "# --------------------------------------------------------------------\n",
        "print(\"--- Loading Full Test Dataset (X and MBTI Y) ---\")\n",
        "try:\n",
        "    # Load all data needed\n",
        "    seq_X_raw = np.load(X_final_path, allow_pickle=True)\n",
        "    seq_y_mbti = np.load(mbti_y_path)\n",
        "    seq_id = np.load(seq_id_path)\n",
        "    N = len(seq_X_raw)\n",
        "\n",
        "    # Pre-process X list and determine input dimension\n",
        "    seq_X = [np.array(x, dtype=np.float32) for x in seq_X_raw]\n",
        "    INPUT_DIM = seq_X[0].shape[1] if N > 0 and seq_X[0].ndim == 2 else 0\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading final data for test: {e}. Exiting.\")\n",
        "    sys.exit()\n",
        "\n",
        "# --- Replicate the exact data split used during training ---\n",
        "dummy_y = np.zeros((N, 5), dtype=np.float32)\n",
        "dataset = OceanDataset(seq_X, dummy_y, seq_id)\n",
        "\n",
        "train_size = int(0.7 * N)\n",
        "val_size   = int(0.15 * N)\n",
        "test_size  = N - train_size - val_size\n",
        "\n",
        "# Split the full dataset to get the test set indices\n",
        "_, _, test_ds_split = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Filter the MBTI ground truth array to match the test set indices\n",
        "test_indices = test_ds_split.indices\n",
        "y_true_mbti_test = seq_y_mbti[test_indices]\n",
        "\n",
        "# Create the final Test Loader using the correct split\n",
        "test_loader_full = DataLoader(test_ds_split, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# --- 3. Load Models ---\n",
        "# --------------------------------------------------------------------\n",
        "print(\"\\n--- Loading Phase 1 (OCEAN) and Phase 2 (MBTI) Models ---\")\n",
        "\n",
        "# Phase 1: BiGRU-Transformer\n",
        "p1_model = OceanEncBiGRUTransformer(input_dim=INPUT_DIM).to(DEVICE)\n",
        "try:\n",
        "    # Load state dict\n",
        "    p1_model.load_state_dict(torch.load(P1_MODEL_PATH, map_location=DEVICE))\n",
        "    p1_model.eval()\n",
        "    print(f\"‚úÖ Loaded Phase 1 Model (BiGRU-T) from {P1_MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading Phase 1 model from {P1_MODEL_PATH}: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "# Phase 2: MLP Classifier\n",
        "# Instantiate with hidden_dim=32 to match the checkpoint size\n",
        "p2_model = MBTIMLPClassifier(input_dim=5, output_dim=4, hidden_dim=32).to(DEVICE)\n",
        "try:\n",
        "    p2_model.load_state_dict(torch.load(P2_MODEL_PATH, map_location=DEVICE))\n",
        "    p2_model.eval()\n",
        "    print(f\"‚úÖ Loaded Phase 2 Model (MLP) from {P2_MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading Phase 2 model from {P2_MODEL_PATH}: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# --- 4. Two-Phase Inference ---\n",
        "# --------------------------------------------------------------------\n",
        "print(\"\\n--- Running Two-Phase Inference on Test Set ---\")\n",
        "predicted_mbti_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader_full:\n",
        "        x, mask = batch[\"x\"].to(DEVICE), batch[\"mask\"].to(DEVICE)\n",
        "\n",
        "        # PHASE 1: Predict OCEAN scores (5 continuous values)\n",
        "        predicted_ocean_scores = p1_model(x, mask)\n",
        "\n",
        "        # PHASE 2: Predict MBTI traits (4 binary labels)\n",
        "        predicted_mbti_probs = p2_model(predicted_ocean_scores)\n",
        "\n",
        "        # Convert probabilities (0-1) to binary predictions (0 or 1)\n",
        "        predicted_mbti = (predicted_mbti_probs > 0.5).float().cpu().numpy()\n",
        "        predicted_mbti_list.append(predicted_mbti)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# --- 5. Final Evaluation ---\n",
        "# --------------------------------------------------------------------\n",
        "y_pred_mbti_test = np.vstack(predicted_mbti_list)\n",
        "y_true_mbti_test = y_true_mbti_test.astype(int)\n",
        "y_pred_mbti_test = y_pred_mbti_test.astype(int)\n",
        "\n",
        "# Compute classification metrics\n",
        "final_subset_accuracy = accuracy_score(y_true_mbti_test, y_pred_mbti_test)\n",
        "\n",
        "# --- CALCULATE TRAIT-WISE ACCURACY ---\n",
        "trait_accuracies = []\n",
        "for i in range(y_true_mbti_test.shape[1]): # Loop through the 4 traits/columns\n",
        "    trait_acc = accuracy_score(y_true_mbti_test[:, i], y_pred_mbti_test[:, i])\n",
        "    trait_accuracies.append(trait_acc)\n",
        "\n",
        "final_avg_trait_accuracy = np.mean(trait_accuracies)\n",
        "\n",
        "final_f1_macro = f1_score(y_true_mbti_test, y_pred_mbti_test, average='macro', zero_division=0)\n",
        "final_precision = precision_score(y_true_mbti_test, y_pred_mbti_test, average='macro', zero_division=0)\n",
        "final_recall = recall_score(y_true_mbti_test, y_pred_mbti_test, average='macro', zero_division=0)\n",
        "\n",
        "\n",
        "print(\"\\n================= FINAL END-TO-END MBTI RESULTS =================\")\n",
        "print(f\"Total Test Samples: {len(y_true_mbti_test)}\")\n",
        "print(f\"Accuracy (Total Label Match - Subset): {final_subset_accuracy:.4f}\")\n",
        "print(f\"Accuracy (AVERAGE TRAIT-WISE): {final_avg_trait_accuracy:.4f}\")\n",
        "print(f\"F1 Score (Macro, per trait): {final_f1_macro:.4f}\")\n",
        "print(f\"Precision (Macro): {final_precision:.4f}\")\n",
        "print(f\"Recall (Macro): {final_recall:.4f}\")\n",
        "print(\"===============================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ez7b9R5Mn0p",
        "outputId": "429f963f-0901-4ad2-f970-ca3874e06a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Full Test Dataset (X and MBTI Y) ---\n",
            "\n",
            "--- Loading Phase 1 (OCEAN) and Phase 2 (MBTI) Models ---\n",
            "‚úÖ Loaded Phase 1 Model (BiGRU-T) from /content/drive/MyDrive/NNDL/Features/ocean_enc_bigru_trans_best.pth\n",
            "‚úÖ Loaded Phase 2 Model (MLP) from /content/drive/MyDrive/NNDL/Features/best_mbti_mlp(1).pth\n",
            "\n",
            "--- Running Two-Phase Inference on Test Set ---\n",
            "\n",
            "================= FINAL END-TO-END MBTI RESULTS =================\n",
            "Total Test Samples: 150\n",
            "Accuracy (Total Label Match - Subset): 0.4000\n",
            "Accuracy (AVERAGE TRAIT-WISE): 0.7367\n",
            "F1 Score (Macro, per trait): 0.7810\n",
            "Precision (Macro): 0.6934\n",
            "Recall (Macro): 0.9100\n",
            "===============================================================\n"
          ]
        }
      ]
    }
  ]
}